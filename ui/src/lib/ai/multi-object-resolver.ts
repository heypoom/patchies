/**
 * Multi-object resolution using two-stage AI approach.
 * 1. Router call: Determines which object types to use and how they connect (lightweight)
 * 2. Generator call: Generates the full object configurations (targeted)
 *
 * CRITICAL: Handle IDs are auto-generated by StandardHandle component, NOT hardcoded.
 * See HANDLE_IDS.md for complete documentation of handle ID generation rules.
 * Key point: Handle IDs depend on node implementation, not on what we specify in edges.
 *
 * Handle ID patterns:
 * - Simple nodes (slider, button, toggle, msg, orca): "message-in", "message-out"
 * - Audio objects (dac~, tone~, sampler~): indexed like "audio-in-0", "message-in-1"
 * - Multi-outlet nodes (Hydra with setVideoCount(2,1)): "video-out-0", "video-out-1"
 * - GLSL nodes: "video-out-out" (type="video" with id="out" â†’ type-portDir-id pattern)
 * - IMPORTANT: Some nodes have SINGLE outlets despite multiple uses:
 *   * orca has only ONE message outlet (can't split to multiple samplers)
 *   * To trigger multiple samplers from orca, use separate orca instances
 * - Never use patterns like "message-in-message-in" or "out-0" without type prefix
 */

import { getObjectSpecificInstructions, OBJECT_TYPE_LIST } from './object-descriptions';

// Consolidated logging for AI Multi-Object debugging
class MultiObjectLogger {
	private logs: string[] = [];

	log(message: string, data?: unknown) {
		if (data !== undefined) {
			this.logs.push(`${message}: ${JSON.stringify(data, null, 2)}`);
		} else {
			this.logs.push(message);
		}
	}

	flush() {
		if (this.logs.length === 0) return;

		const consolidated = `
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           [AI Multi-Object] Consolidated Debug Log             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

${this.logs.join('\n\n')}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
`;
		console.log(consolidated);
		this.logs = [];
	}

	error(message: string, error?: unknown) {
		if (error) {
			this.logs.push(
				`âŒ ${message}: ${error instanceof Error ? error.message : JSON.stringify(error)}`
			);
		} else {
			this.logs.push(`âŒ ${message}`);
		}
	}
}

const logger = new MultiObjectLogger();

export type SimplifiedEdge = {
	source: number; // Index of source node in nodes array
	target: number; // Index of target node in nodes array
	sourceHandle?: string; // e.g., 'message-out', 'audio-out', 'in-0', 'video-in-0'
	targetHandle?: string; // e.g., 'message-in', 'audio-in', 'in-1', 'audio-in-0'
};

export type MultiObjectResult = {
	nodes: Array<{
		type: string;
		// eslint-disable-next-line @typescript-eslint/no-explicit-any -- fixme
		data: any;
		position?: { x: number; y: number }; // Optional relative positioning
	}>;
	edges: SimplifiedEdge[];
};

/**
 * Uses Gemini AI to resolve a natural language prompt to multiple connected objects.
 * Uses a two-call approach:
 * 1. Router call: Determines which object types to use and how they connect (lightweight)
 * 2. Generator call: Generates the full object configurations (targeted)
 *
 * Accepts optional callback to report progress between calls for better UX.
 * Supports cancellation via AbortSignal.
 */
export async function resolveMultipleObjectsFromPrompt(
	prompt: string,
	onRouterComplete?: (objectTypes: string[]) => void,
	signal?: AbortSignal
): Promise<MultiObjectResult | null> {
	const apiKey = localStorage.getItem('gemini-api-key');

	if (!apiKey) {
		throw new Error('Gemini API key is not set. Please set it in the settings.');
	}

	// Check for cancellation before starting
	if (signal?.aborted) {
		throw new Error('Request cancelled');
	}

	const { GoogleGenAI } = await import('@google/genai');
	const ai = new GoogleGenAI({ apiKey });

	logger.log('ğŸ“ Starting multi-object resolution');
	logger.log('User prompt', prompt);

	// Call 1: Route to object types and structure (lightweight)
	const plan = await routeToMultiObjectPlan(ai, prompt, signal);
	if (!plan) {
		logger.log('âš ï¸ Router returned no plan');
		logger.flush();
		return null;
	}

	// Check for cancellation after first call
	if (signal?.aborted) {
		throw new Error('Request cancelled');
	}

	// Report router completion to UI
	onRouterComplete?.(plan.objectTypes);

	// Call 2: Generate full object configs (targeted)
	const result = await generateMultiObjectConfig(ai, prompt, plan, signal);

	logger.flush();

	return result;
}

/**
 * Call 1 (Multi-object): Routes to object types and determines connection structure.
 * This is a lightweight call that only includes object descriptions.
 */
async function routeToMultiObjectPlan(
	ai: InstanceType<typeof import('@google/genai').GoogleGenAI>,
	prompt: string,
	signal?: AbortSignal
): Promise<{ objectTypes: string[]; structure: string } | null> {
	// Check for cancellation before starting
	if (signal?.aborted) {
		throw new Error('Request cancelled');
	}

	const routerPrompt = buildMultiObjectRouterPrompt();

	const response = await ai.models.generateContent({
		model: 'gemini-3-flash-preview',
		contents: [{ text: `${routerPrompt}\n\nUser prompt: "${prompt}"` }]
	});

	// Check for cancellation after request
	if (signal?.aborted) {
		throw new Error('Request cancelled');
	}

	const responseText = response.text?.trim();
	if (!responseText) {
		logger.log('âš ï¸ Router response is empty');
		return null;
	}

	try {
		// Extract JSON from response (handle markdown code blocks)
		const jsonMatch = responseText.match(/```(?:json)?\s*(\{[\s\S]*?\})\s*```/);
		const jsonText = jsonMatch ? jsonMatch[1] : responseText;

		const result = JSON.parse(jsonText);

		if (!result.objectTypes || !Array.isArray(result.objectTypes)) {
			throw new Error('Response missing required "objectTypes" array');
		}

		if (!result.structure) {
			throw new Error('Response missing required "structure" field');
		}

		logger.log('âœ… [Router] Object types', result.objectTypes);
		logger.log('âœ… [Router] Connection structure', result.structure);

		return result;
	} catch (error) {
		logger.error('[Router] Failed to parse response', error);
		logger.log('Raw response text', responseText);
		throw new Error('Failed to parse routing response as JSON');
	}
}

/**
 * Call 2 (Multi-object): Generates full object configurations based on the plan.
 * This is a targeted call that includes only relevant system prompts.
 */
async function generateMultiObjectConfig(
	ai: InstanceType<typeof import('@google/genai').GoogleGenAI>,
	prompt: string,
	plan: { objectTypes: string[]; structure: string },
	signal?: AbortSignal
): Promise<MultiObjectResult | null> {
	// Check for cancellation before starting
	if (signal?.aborted) {
		throw new Error('Request cancelled');
	}

	const systemPrompt = buildMultiObjectGeneratorPrompt(plan.objectTypes, plan.structure);

	const response = await ai.models.generateContent({
		model: 'gemini-3-flash-preview',
		contents: [{ text: `${systemPrompt}\n\nUser prompt: "${prompt}"` }]
	});

	// Check for cancellation after request
	if (signal?.aborted) {
		throw new Error('Request cancelled');
	}

	const responseText = response.text?.trim();
	if (!responseText) {
		logger.log('âš ï¸ Generator response is empty');
		return null;
	}

	try {
		// Extract JSON from response (handle markdown code blocks)
		const jsonMatch = responseText.match(/```(?:json)?\s*(\{[\s\S]*?\})\s*```/);
		const jsonText = jsonMatch ? jsonMatch[1] : responseText;

		const result = JSON.parse(jsonText);

		// Validate the result has required fields
		if (!result.nodes || !Array.isArray(result.nodes)) {
			throw new Error('Response missing required "nodes" array');
		}

		if (!result.edges || !Array.isArray(result.edges)) {
			throw new Error('Response missing required "edges" array');
		}

		// Validate each node has a type
		for (const node of result.nodes) {
			if (!node.type) {
				throw new Error('Node missing required "type" field');
			}
		}

		logger.log('âœ… [Generator] Successfully parsed result');
		logger.log('âœ… [Generator] Nodes created', result.nodes);
		logger.log('âœ… [Generator] Edges created', result.edges);

		return {
			nodes: result.nodes,
			edges: result.edges
		};
	} catch (error) {
		logger.error('[Generator] Failed to parse response', error);
		logger.log('Raw response text', responseText);
		throw new Error('Failed to parse generation response as JSON');
	}
}

/**
 * Builds the multi-object router prompt - lightweight, only object descriptions
 */
function buildMultiObjectRouterPrompt(): string {
	return `You are an AI assistant that plans multi-object configurations in Patchies, a visual patching environment for creative coding.

Your task: Read the user's prompt and determine:
1. Which object types are needed
2. How they should connect (structure description)

Return ONLY a JSON object with this format:
{
  "objectTypes": ["type1", "type2", ...],
  "structure": "brief description of connections (e.g., 'slider controls osc~ frequency')"
}

AVAILABLE OBJECT TYPES:

${OBJECT_TYPE_LIST}

EXAMPLES:

User: "slider controlling oscillator frequency"
Response:
{
  "objectTypes": ["slider", "tone~"],
  "structure": "slider connects to tone~ oscillator frequency inlet"
}

User: "button that triggers a visual animation"
Response:
{
  "objectTypes": ["button", "p5"],
  "structure": "button sends bang to p5 sketch to trigger animation"
}

User: "XY pad controlling two parameters"
Response:
{
  "objectTypes": ["canvas.dom", "expr", "expr"],
  "structure": "XY pad outputs [x, y] array, split by two expr objects for separate parameter control"
}

User: "808 drum machine with kick and snare"
Response:
{
  "objectTypes": ["button", "button", "tone~", "tone~", "object"],
  "structure": "Two buttons trigger two tone~ drum sounds (kick and snare), both connect to object (dac~) for speaker output"
}

IMPORTANT: For audio output to speakers, use "object" type (which will create dac~), NOT "dac~" as a direct type.

Now analyze this prompt:`;
}

/**
 * Builds the multi-object generator prompt - targeted for specific object types
 */
function buildMultiObjectGeneratorPrompt(objectTypes: string[], structure: string): string {
	// Get object-specific instructions for each type
	const objectInstructions = objectTypes
		.map((type) => getObjectSpecificInstructions(type))
		.join('\n\n---\n\n');

	return `You are an AI assistant that generates multiple connected object configurations in Patchies, a visual patching environment for creative coding.

Your task: Create a complete multi-object configuration based on the plan.

PLAN:
- Object types needed: ${objectTypes.join(', ')}
- Structure: ${structure}

IMPORTANT RULES:
1. You MUST respond with ONLY a valid JSON object, nothing else
2. The JSON must have a "nodes" array and an "edges" array
3. Each node in the "nodes" array must have a "type" field and a "data" field
4. Each node SHOULD have a "position" field with relative x, y coordinates for good visual layout
5. Position nodes in a TOP-TO-BOTTOM flow (like Pd): sources on top (y: 0), outputs at the bottom (y: 160+)
6. Use plenty of horizontal spacing (x: 0, x: 350, x: 700, x: 1050) for side-by-side or parallel nodes, so they don't overlap.
7. Vertical steps should be at least 160-200 pixels between rows for a more compact layout
8. Calculate spacing based on signal flow depth: sources at y=0, first processing stage y=180, next y=360, outputs y=540+
7. Each edge in the "edges" array connects nodes by their index in the nodes array
8. Edges use "source" (node index), "target" (node index), and optionally "sourceHandle" and "targetHandle"
9. CRITICAL - Handle ID Generation Rules:
   Handle IDs are auto-generated by the StandardHandle component based on node implementation.
   DO NOT hardcode handle IDs. Instead, understand what IDs each node type will generate:

   SIMPLE NODES (single port of each type): slider, button, toggle, msg, textbox
   - Will generate: "message-in", "message-out", "audio-in", "audio-out", "video-in", "video-out"
   - Example edges: sourceHandle: "message-out", targetHandle: "message-in"
   
   TONE~ NODES (SimpleDspLayout with dynamic message ports):
   - Audio inlet (always): "audio-in" (index 0)
   - Audio outlet (always): "audio-out" (index 0)
   - Message inlets (when setPortCount(n) where n > 0): ALWAYS indexed as "message-in-0", "message-in-1", etc.
   - CRITICAL: Even with setPortCount(1), use "message-in-0" (NOT "message-in")
   - Example: button â†’ tone~ with setPortCount(1): targetHandle: "message-in-0"
   - Example: tone~ â†’ dac~: sourceHandle: "audio-out", targetHandle: "audio-in-0"
   
   BACKGROUND OUTPUT (bg.out):
   - Has one video inlet with explicit id="0": generates "video-in-0"
   - To connect to bg.out: targetHandle: "video-in-0"
   - Example: hydra â†’ bg.out: sourceHandle: "video-out-0", targetHandle: "video-in-0"

   NODES WITH EXPLICIT LABEL IDS (e.g., sampler~, soundfile~):
   - sampler~: has explicit id="audio-in", id="message-in", id="audio-out"
     * Pattern: "{type}-{direction}-{id}"
     * These generate: "audio-in-audio-in", "message-in-message-in", "audio-out-audio-out"
     * When connecting TO sampler~: targetHandle: "message-in-message-in" (NOT "message-in")
     * Example: button â†’ sampler~: sourceHandle: "message-out", targetHandle: "message-in-message-in"
   - soundfile~: has id="0" on audio outlet only
     * Generates: "message-in" (no explicit id), "audio-out-0" (explicit id)

   NUMBERED/INDEXED NODES (multiple ports): ObjectNode (audio objects), Hydra, HydraCanvas
   - Will generate: "in-0", "in-1", "out-0", "out-1" (numeric indices)
   - OR: "audio-in-0", "message-in-1", "video-out-0" (type-specific)
   - For ObjectNode with multiple inlets/outlets, handles are indexed: "in-0", "in-1", etc.
   - For Hydra with 2 video inlets + 1 message inlet: "video-in-0", "video-in-1", "message-in-2"
   - The "object" type creates audio/control objects dynamically:
     * object type: gain~, osc~, delay~, filter~, etc. (from expression string)
     * Auto-generates handles based on object type (e.g., gain~ has "audio-in-0", "audio-out-0")
     * Consult HANDLE_IDS.md for specific audio object patterns
   
   CRITICAL: Audio Objects via "object" Node Type
   - Audio processing objects (dac~, gain~, osc~, filter~, etc.) are NOT direct node types
   - To create them, use type: "object" with data: { expr: "objectName", name: "objectName", params: [] }
   - Example dac~ node: { "type": "object", "data": { "expr": "dac~", "name": "dac~", "params": [] } }
   - Example gain~ node: { "type": "object", "data": { "expr": "gain~ 0.5", "name": "gain~", "params": [0.5] } }
   - These objects generate handles like "audio-in-0", "audio-out-0" (indexed)
   
   CRITICAL: dac~ (Digital-to-Analog Converter - speakers output)
   - dac~ created as: { "type": "object", "data": { "expr": "dac~", "name": "dac~", "params": [] } }
   - dac~ has ONLY ONE audio inlet: "audio-in-0"
   - MULTIPLE audio sources CAN and SHOULD connect to the SAME "audio-in-0" handle
   - Web Audio automatically sums/mixes multiple connections to the same inlet
   - Example: 6 drum sounds â†’ all connect to dac~ with targetHandle: "audio-in-0"
   - DO NOT create separate dac~ nodes for each source
   - DO NOT try to connect to "audio-in-1", "audio-in-2", etc. (they don't exist!)
   - Pattern for multi-source audio output:
     * drum1 â†’ dac~ (targetHandle: "audio-in-0")
     * drum2 â†’ dac~ (targetHandle: "audio-in-0")
     * drum3 â†’ dac~ (targetHandle: "audio-in-0")
     * All sounds will be mixed and output to speakers
   
   SPECIAL DYNAMIC NODES: GLSLCanvasNode, P5Node, HydraNode
   - GLSL nodes (type: "glsl"):
     * Video outlet generates: "video-out-out" (type="video" with id="out")
     * When connecting FROM glsl to another node: sourceHandle: "video-out-out"
     * Shader uniforms (inlets) generate: "video-in-{index}-{uniformName}-{type}"
   - Do NOT specify uniform handles in edges; let the node handle it
   - If connecting to GLSL uniforms, omit targetHandle to let framework match

   NODES WITH SINGLE OUTLETS (IMPORTANT for multi-sampler patterns):
   - orca: has ONLY ONE message outlet (not multiple!)
     * To trigger multiple samplers, you MUST use separate orca instances per sampler
     * OR use midi.out node which can split MIDI data to multiple receivers
     * WRONG: one orca â†’ three samplers (orca has single outlet, can't split)
     * RIGHT: three separate orca instances â†’ three samplers
   - strudel: similar to orca, single message outlet
   - button, toggle, slider: single message outlets

   AVOID HARDCODED HANDLE IDS:
   - WRONG: sourceHandle: "audio-in-audio-in" (doesn't auto-generate from StandardHandle)
   - WRONG: targetHandle: "message-in-message-in" (incorrect pattern)
   - WRONG: sourceHandle: "out-0" (missing type prefix)
   - RIGHT: sourceHandle: "audio-out" (for nodes with single audio outlet)
   - RIGHT: sourceHandle: "message-out" (for nodes with single message outlet)
   - RIGHT: targetHandle: "audio-in-0" (for audio objects with indexed inlets)
   - RIGHT: omit sourceHandle/targetHandle for untyped connections (they auto-match)

10. Focus on creating FUNCTIONAL, CONNECTED systems
11. ALWAYS include appropriate code and helper functions for each object type
12. CRITICAL for shader graphs: Space GLSL nodes generously (y: 0, y: 350, y: 700, y: 1050) with 350+ pixel gaps
13. Multiple GLSL shaders in sequence should be horizontally offset: x: 0, x: 350, x: 700 to reduce visual clutter

RESPONSE FORMAT:
{
  "nodes": [
    {
      "type": "objectType",
      "data": { /* object configuration */ },
      "position": { "x": 0, "y": 0 }
    }
  ],
  "edges": [
    {
      "source": 0,
      "target": 1,
      "sourceHandle": "message-out",
      "targetHandle": "message-in"
    }
  ]
}

LAYOUT EXAMPLE (top-to-bottom like Pd with generous spacing):
- slider at top: { "position": { "x": 0, "y": 0 } }
- tone~ below: { "position": { "x": 0, "y": 300 } } (300px gap minimum!)
- If parallel inputs, use GENEROUS horizontal spacing:
  - slider1: { "x": 0, "y": 0 }
  - slider2: { "x": 350, "y": 0 } (350px apart minimum)
  - button: { "x": 700, "y": 0 } (700px from slider1)
  - all connect to processor below: { "x": 350, "y": 300 } (centered, 300px down)

OBJECT-SPECIFIC INSTRUCTIONS:

${objectInstructions}

Now generate the multi-object configuration.`;
}
